## High-Performance Modelling : 
> Implement distributed processing and parallelism techniques to make the most of your computational resources for training your models efficiently.
**Learning Objectives**
  + Identify the rise in computational requirements with current network architectures
  + Select techniques to optimize the usage of computational resources to best serve your model needs
  + Carry out high-performance data ingestion to reduce hardware accelerators idling time
  + Distinguish between data and model parallelism to train your models in the most efficient way
  + Implement knowledge distillation to reduce models that capture complex relationships among features so that they fit in constrained deployment environments
### Types of distributed training
+ Data parallelism: In data parallelism, models are replicated onto
different accelerators (GPU/TPU) and data is split between them
+    Model parallelism: When models are too large to ﬁt on a single device
then they can be divided into partitions, assigning different partitions to
different accelerators

## Click to more information
+ **[ETL process in Machine Learning](https://viblo.asia/p/etl-va-elt-nhung-su-khac-biet-can-phai-biet-Ljy5VQGVlra)**

+ **[How to optimize pipeline performance?](https://www.superannotate.com/blog/how-to-optimize-machine-learning-pipeline)**

+ **[important layers in Deep Learning](https://phamdinhkhanh.github.io/2019/12/02/DeepLearningLayer.html)**

+ Data ingestion (nhập dữ liệu) là quá trình mà dữ liệu được tải từ nhiều nguồn khác nhau vào phương tiện lưu trữ, chẳng hạn như kho dữ liệu – nơi dữ liệu có thể được truy cập, sử dụng và phân tích.

+ Google Kubernetes Engine (GKE) – là một hệ thống quản lý và điều phối cho vùng chứa Docker và các cụm vùng chứa chạy trong các dịch vụ đám mây công cộng của Google. Google Kubernetes Engine dựa trên Kubernetes, hệ thống quản lý vùng chứa mã nguồn mở của Google.
+ **[Tổng quan về hạ tầng phần cứng cho Deep Learning ở quy mô lớn](https://www.thegioimaychu.vn/blog/ai-hpc/tong-quan-ve-ha-tang-phan-cung-cho-deep-learning-o-quy-mo-lon-p7497/)**

## Training Large Models - The Rise of Giant Neural Nets and Parallelism
+ **[Knowledge Distillation](https://viblo.asia/p/knowledge-distillation-chat-loc-tri-thuc-tu-nhung-mo-hinh-thanh-cong-naQZRBYjZvx)**
